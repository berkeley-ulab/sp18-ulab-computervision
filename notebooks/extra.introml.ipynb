{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Introduction to Machine Learning\n",
    "===============\n",
    "\n",
    "Data\n",
    "--------\n",
    "We are given data $n$ samples of data: a collection of features and their corresponding labels. The data is typically separated into to matrices $X$ and $y$ of size $n \\times d$ and $n \\times 1$, respectively, where $d$ is the number of features per data point.\n",
    "\n",
    "For example, if I am trying to build a model to predict house prices, I could collect data on the # of bathrooms, # of bedrooms, total square feet, etc. These are all examples of features. \n",
    "\n",
    "Typically, we will split the data into a training set and a test set. We train the model using the training set, and then evaluate how well the model does using the test set. This is to make sure that our model is not \"memorizing\" the training set and is able to generalize well to data it has not seen before.\n",
    "\n",
    "Model\n",
    "--------\n",
    "There are many models to choose from in ML. We will just be going over three main ones - linear regression, fully connected neural nets, and convolutional neural nets. A model just takes in some input, and outputs a number (or many numbers). \n",
    "\n",
    "For example, going back to the house price prediction example, the model would take in a list of features, and output a predicted price.\n",
    "\n",
    "Loss Function\n",
    "--------\n",
    "The loss function provides a way to measure how accurate/inaccurate the model is. For regression, we typically use squared error, defined as\n",
    "$$\\text{error} = \\frac{1}{2}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "where $n$ is the number of training examples, $y_i$ is the true label for the $i$th sample, and $\\hat{y}_i$ is the predicted label for the $i$th sample. We want to optimize the model to have a low squared error.\n",
    "\n",
    "Parameter Updates\n",
    "--------\n",
    "Let's say that our model is a linear model, i.e. $$\\hat{y} = wx + b$$ $w$ and $b$ are initialized randomly, and we want to adjust these parameters to fit the true model of the data. Since we are using MSE as an intermediary to measure how accurate the model is, we can adjust the parameters to decrease the loss. This is where derivatives come in. The goal is to calculate \n",
    "\\begin{align*}\n",
    "&\\frac{\\partial L}{\\partial w} = (\\hat{y}_i - y_i)x_i\\\\\n",
    "&\\frac{\\partial L}{\\partial b} = \\hat{y}_i - y_i\n",
    "\\end{align*}\n",
    "Now, we can adjust our paramters to decrease the loss:\n",
    "\\begin{align*}\n",
    "&w = w - \\alpha\\frac{\\partial L}{\\partial w} \\\\\n",
    "&b = b - \\alpha\\frac{\\partial L}{\\partial b}\n",
    "\\end{align*}\n",
    "where $\\alpha$ is called the learning rate. $\\alpha$ is commonly some small value, such as $1e-1, 1e-2, 1e-5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear Regression\n",
    "===============\n",
    "\n",
    "A linear models is one of the simplest models in machine learning. In this section, we will try to fit a few different functions to a lineaer model, and see how well they approximate them. A linear model can be written as $$\\hat{y} = wx + b$$\n",
    "where $\\hat{y}$ is the prediction (output) and $x$ is the feature (input). In this case, we are trying to learn the correct values of $w$ (weight) and $b$ (bias).\n",
    "\n",
    "Although we can solve a linear regression problem using a provided formula, we will not use is in this case. Instead, we will apply a more general approach to solving an optimization problem (minimizing mean-squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearModel(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(LinearModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc1(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(X, y, model, criterion, optimizer, iters=10):\n",
    "    \"\"\"\n",
    "    Trains the provided model given data, a loss function, and an optimizer. \n",
    "    Can specify the number of iterations to train for.\n",
    "    \"\"\"\n",
    "    losses = []\n",
    "    X, y = Variable(X, requires_grad=False), Variable(y, requires_grad=False)\n",
    "\n",
    "    for _ in range(iters):\n",
    "        out = model(X) # run the input through the model\n",
    "        loss = criterion(out, y) # calculate the loss between predicted y and true y\n",
    "        optimizer.zero_grad() # zero out the gradients - VERY IMPORTANT\n",
    "        loss.backward() # calculate the gradients of the loss with respect to the parameters\n",
    "        optimizer.step() # adjust weights according to gradient\n",
    "        \n",
    "        losses.append(loss.data[0])\n",
    "    return model, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot(X, y, model=None):\n",
    "    \"\"\"\n",
    "    Plots the given data. If a model is given, the predicted model will also be shown.\n",
    "    \"\"\"\n",
    "    plt.scatter(X.numpy(), y.numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('data')\n",
    "    \n",
    "    if model:\n",
    "        xt = torch.linspace(torch.min(X), torch.max(X), 10)\n",
    "        xt = xt.view(10, 1)\n",
    "        yt = model(Variable(xt))\n",
    "        plt.plot(xt.numpy(), yt.data.numpy(), c='r')\n",
    "        plt.title('data + model learned')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_poly(X, y, model=None):\n",
    "    plt.scatter(X.numpy(), y.numpy())\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('data')\n",
    "    \n",
    "    if model:\n",
    "        xt = torch.linspace(torch.min(X), torch.max(X), 10)\n",
    "        xt = xt.view(10, 1)\n",
    "        yt = model(Variable(torch.cat((xt, xt ** 2), 1)))\n",
    "        plt.plot(xt.numpy(), yt.data.numpy(), c='r')\n",
    "        plt.title('data + model learned') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w, b = 2.3, 0.0125\n",
    "x = torch.linspace(-10, 10, 10) # generates 10 evenly-spaced values from 0 to 10\n",
    "x = x.view(10, 1)\n",
    "y = w * x + b + torch.randn(x.size())\n",
    "\n",
    "plot(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "\n",
    "model, losses = train(x, y, model, criterion, optimizer, iters=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(x, y, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to approximate a more complicated function. Let $f(x) = x^2 + x - 5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Things to do:\n",
    "1) Generate the data (see sample code above) - don't forget to add noise!\n",
    "2) Plot the data (see plot function)\n",
    "3) Create a new LinearModel instance and train it using your generated data\n",
    "4) Visualize the loss vs iterations\n",
    "5) Visualize your predicted model (see plot function - include the optional argument)\n",
    "\"\"\"\n",
    "x = torch.linspace(-10, 10, 10)\n",
    "x = x.view(10, 1)\n",
    "\n",
    "\"\"\" Your Code Here \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may notice that after training your model, it does not fit the given data very well. Let's try adding in a new feature $x^2$, so that your model is:\n",
    "$$\\hat{y} = w_1x + w_2x^2 + b$$\n",
    "Note that we are now learning three parameters: $w_1$, $w_2$, and $b$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearModel(2)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-5)\n",
    "model, losses = train(torch.cat((x, x ** 2), 1), y, model, criterion, optimizer, iters=100)\n",
    "plot_poly(x, y, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses)\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('MSE Loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Neural Networks\n",
    "===============\n",
    "\n",
    "Neural networks can be constructed using the ``torch.nn`` package.\n",
    "\n",
    "Now that you had a glimpse of ``autograd``, ``nn`` depends on\n",
    "``autograd`` to define models and differentiate them.\n",
    "An ``nn.Module`` contains layers, and a method ``forward(input)``\\ that\n",
    "returns the ``output``.\n",
    "\n",
    "For example, look at this network that classifies digit images:\n",
    "\n",
    ".. figure:: /_static/img/mnist.png\n",
    "   :alt: convnet\n",
    "\n",
    "   convnet\n",
    "\n",
    "It is a simple feed-forward network. It takes the input, feeds it\n",
    "through several layers one after the other, and then finally gives the\n",
    "output.\n",
    "\n",
    "A typical training procedure for a neural network is as follows:\n",
    "\n",
    "- Define the neural network that has some learnable parameters (or\n",
    "  weights)\n",
    "- Iterate over a dataset of inputs\n",
    "- Process input through the network\n",
    "- Compute the loss (how far is the output from being correct)\n",
    "- Propagate gradients back into the network’s parameters\n",
    "- Update the weights of the network, typically using a simple update rule:\n",
    "  ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "Define the network\n",
    "------------------\n",
    "\n",
    "Let’s define this network:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FCNet, self).__init__()\n",
    "        # 1 input flattened 28 x 28 image (so one vector of 784 elements)\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(28 * 28, 256)\n",
    "        self.fc2 = nn.Linear(256, 256)\n",
    "        self.fc3 = nn.Linear(256, 256)\n",
    "        self.fc4 = nn.Linear(256, 10) # output into 10 categories\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "net = FCNet()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You just have to define the ``forward`` function, and the ``backward``\n",
    "function (where gradients are computed) is automatically defined for you\n",
    "using ``autograd``.\n",
    "You can use any of the Tensor operations in the ``forward`` function.\n",
    "\n",
    "The learnable parameters of a model are returned by ``net.parameters()``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the forward is an ``autograd.Variable``, and so is the output.\n",
    "Note: Expected input size to this net(LeNet) is 32x32. To use this net on\n",
    "MNIST dataset, please resize the images from the dataset to 32x32.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = Variable(torch.randn(1, 1, 28, 28).view(1, -1))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero the gradient buffers of all parameters and backprops with random\n",
    "gradients:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.randn(1, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"><h4>Note</h4><p>``torch.nn`` only supports mini-batches. The entire ``torch.nn``\n",
    "    package only supports inputs that are a mini-batch of samples, and not\n",
    "    a single sample.\n",
    "\n",
    "    For example, ``nn.Conv2d`` will take in a 4D Tensor of\n",
    "    ``nSamples x nChannels x Height x Width``.\n",
    "\n",
    "    If you have a single sample, just use ``input.unsqueeze(0)`` to add\n",
    "    a fake batch dimension.</p></div>\n",
    "\n",
    "Before proceeding further, let's recap all the classes you’ve seen so far.\n",
    "\n",
    "**Recap:**\n",
    "  -  ``torch.Tensor`` - A *multi-dimensional array*.\n",
    "  -  ``autograd.Variable`` - *Wraps a Tensor and records the history of\n",
    "     operations* applied to it. Has the same API as a ``Tensor``, with\n",
    "     some additions like ``backward()``. Also *holds the gradient*\n",
    "     w.r.t. the tensor.\n",
    "  -  ``nn.Module`` - Neural network module. *Convenient way of\n",
    "     encapsulating parameters*, with helpers for moving them to GPU,\n",
    "     exporting, loading, etc.\n",
    "  -  ``nn.Parameter`` - A kind of Variable, that is *automatically\n",
    "     registered as a parameter when assigned as an attribute to a*\n",
    "     ``Module``.\n",
    "  -  ``autograd.Function`` - Implements *forward and backward definitions\n",
    "     of an autograd operation*. Every ``Variable`` operation, creates at\n",
    "     least a single ``Function`` node, that connects to functions that\n",
    "     created a ``Variable`` and *encodes its history*.\n",
    "\n",
    "**At this point, we covered:**\n",
    "  -  Defining a neural network\n",
    "  -  Processing inputs and calling backward\n",
    "\n",
    "**Still Left:**\n",
    "  -  Computing the loss\n",
    "  -  Updating the weights of the network\n",
    "\n",
    "Loss Function\n",
    "-------------\n",
    "A loss function takes the (output, target) pair of inputs, and computes a\n",
    "value that estimates how far away the output is from the target.\n",
    "\n",
    "There are several different\n",
    "`loss functions <http://pytorch.org/docs/nn.html#loss-functions>`_ under the\n",
    "nn package .\n",
    "A simple loss is: ``nn.MSELoss`` which computes the mean-squared error\n",
    "between the input and the target.\n",
    "\n",
    "For example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = net(input)\n",
    "target = Variable(torch.arange(1, 11))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if you follow ``loss`` in the backward direction, using its\n",
    "``.grad_fn`` attribute, you will see a graph of computations that looks\n",
    "like this:\n",
    "\n",
    "::\n",
    "\n",
    "    input -> lienar -> relu -> linear -> relu -> linear -> relu\n",
    "          -> linear -> MSELoss -> loss\n",
    "\n",
    "So, when we call ``loss.backward()``, the whole graph is differentiated\n",
    "w.r.t. the loss, and all Variables in the graph will have their\n",
    "``.grad`` Variable accumulated with the gradient.\n",
    "\n",
    "For illustration, let us follow a few steps backward:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(loss.grad_fn)  # MSELoss\n",
    "print(loss.grad_fn.next_functions[0][0])  # Linear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Backprop\n",
    "--------\n",
    "To backpropagate the error all we have to do is to ``loss.backward()``.\n",
    "You need to clear the existing gradients though, else gradients will be\n",
    "accumulated to existing gradients.\n",
    "\n",
    "\n",
    "Now we shall call ``loss.backward()``, and have a look at conv1's bias\n",
    "gradients before and after the backward.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "\n",
    "print('fc1.bias.grad before backward')\n",
    "print(net.fc1.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('fc2.bias.grad after backward')\n",
    "print(net.fc2.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we have seen how to use loss functions.\n",
    "\n",
    "**Read Later:**\n",
    "\n",
    "  The neural network package contains various modules and loss functions\n",
    "  that form the building blocks of deep neural networks. A full list with\n",
    "  documentation is `here <http://pytorch.org/docs/nn>`_.\n",
    "\n",
    "**The only thing left to learn is:**\n",
    "\n",
    "  - Updating the weights of the network\n",
    "\n",
    "Update the weights\n",
    "------------------\n",
    "The simplest update rule used in practice is the Stochastic Gradient\n",
    "Descent (SGD):\n",
    "\n",
    "     ``weight = weight - learning_rate * gradient``\n",
    "\n",
    "We can implement this using simple python code:\n",
    "\n",
    ".. code:: python\n",
    "\n",
    "    learning_rate = 0.01\n",
    "    for f in net.parameters():\n",
    "        f.data.sub_(f.grad.data * learning_rate)\n",
    "\n",
    "However, as you use neural networks, you want to use various different\n",
    "update rules such as SGD, Nesterov-SGD, Adam, RMSProp, etc.\n",
    "To enable this, we built a small package: ``torch.optim`` that\n",
    "implements all these methods. Using it is very simple:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad()   # zero the gradient buffers\n",
    "output = net(input)\n",
    "loss = criterion(output, target)\n",
    "loss.backward()\n",
    "optimizer.step()    # Does the update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. Note::\n",
    "\n",
    "      Observe how gradient buffers had to be manually set to zero using\n",
    "      ``optimizer.zero_grad()``. This is because gradients are accumulated\n",
    "      as explained in `Backprop`_ section.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on MNIST\n",
    "==========\n",
    "\n",
    "Now, we'll try to train this fully connected neural network on the MNIST dataset. In MNIST, we are given 32x32 images of numbers and asked to classify them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "lr = 1e-2\n",
    "epochs = 4\n",
    "log_interval = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,)),\n",
    "])\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "    datasets.MNIST('data/', train=True, download=True,\n",
    "                   transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "    datasets.MNIST('data/', train=False, transform=transform),\n",
    "    batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing the Data\n",
    "\n",
    "It's always a good sanity check to see sample sample data in yoru dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = next(iter(train_loader))\n",
    "img = X[0].squeeze(0)\n",
    "label = str(y[0])\n",
    "plt.title(label)\n",
    "plt.imshow(img.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = FCNet()\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (X, y) in enumerate(train_loader):\n",
    "        # Fill out the rest of this function. The input for X is 32 x 28 x 28,\n",
    "        # you will need to reshape it to 32 x 784 (see torch.Tensor.view)\n",
    "        \"\"\" Your Code Here \"\"\"\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(X), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test():\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for X, y in test_loader:\n",
    "        X = X.view(X.size(0), -1)\n",
    "        X, y = Variable(X, volatile=True), Variable(y)\n",
    "        out = model(X)\n",
    "        test_loss += criterion(out, y).data[0] # sum up batch loss\n",
    "        pred = out.data.max(1, keepdim=True)[1] # get the index of the max log-probability\n",
    "        correct += pred.eq(y.data.view_as(pred)).long().cpu().sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
