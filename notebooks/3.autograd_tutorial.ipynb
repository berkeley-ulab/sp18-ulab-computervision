{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Autograd: automatic differentiation\n",
    "===================================\n",
    "\n",
    "Central to all neural networks in PyTorch is the ``autograd`` package.\n",
    "Let’s first briefly visit this, and we will then go to training our\n",
    "first neural network.\n",
    "\n",
    "\n",
    "The ``autograd`` package provides automatic differentiation for all operations\n",
    "on Tensors. It is a define-by-run framework, which means that your backprop is\n",
    "defined by how your code is run, and that every single iteration can be\n",
    "different.\n",
    "\n",
    "Let us see this in more simple terms with some examples.\n",
    "\n",
    "Variable\n",
    "--------\n",
    "\n",
    "``autograd.Variable`` is the central class of the package. It wraps a\n",
    "Tensor, and supports nearly all of operations defined on it. Once you\n",
    "finish your computation you can call ``.backward()`` and have all the\n",
    "gradients computed automatically.\n",
    "\n",
    "You can access the raw tensor through the ``.data`` attribute, while the\n",
    "gradient w.r.t. this variable is accumulated into ``.grad``.\n",
    "\n",
    "<img src=\"tutorial_img/variable.png\">\n",
    "\n",
    "   Variable\n",
    "\n",
    "There’s one more class which is very important for autograd\n",
    "implementation - a ``Function``.\n",
    "\n",
    "``Variable`` and ``Function`` are interconnected and build up an acyclic\n",
    "graph, that encodes a complete history of computation. Each variable has\n",
    "a ``.grad_fn`` attribute that references a ``Function`` that has created\n",
    "the ``Variable`` (except for Variables created by the user - their\n",
    "``grad_fn is None``).\n",
    "\n",
    "If you want to compute the derivatives, you can call ``.backward()`` on\n",
    "a ``Variable``. If ``Variable`` is a scalar (i.e. it holds a one element\n",
    "data), you don’t need to specify any arguments to ``backward()``,\n",
    "however if it has more elements, you need to specify a ``gradient``\n",
    "argument that is a tensor of matching shape.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you perform operations on a Variable object, Pytorch will create a computation graph that remembers the past inputs and actions you took. See the example graph below\n",
    "<img src=\"tutorial_img/comp_graph.png\" width=\"50%\">\n",
    "\n",
    "The inputs are $a$ and $b$. $c$ and $d$ are intermediate values, and $e$ is the output. Let's see what this looks like in code! Note that all of the nodes we will create will be $\\texttt{Variable}$ objects, so if you want to extract the actual value, you need to call something like $\\texttt{a.data[0]}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we create the $a$ and $b$ nodes. Since they are inputs, we initialize them as $\\texttt{Variable}$. This creates the bottom two nodes in the graph above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = Variable(torch.FloatTensor([2]), requires_grad=True)\n",
    "b = Variable(torch.FloatTensor([1]), requires_grad=True)\n",
    "print('a=%s, b=%s' % (a.data[0], b.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create the intermediate nodes $c$ and $d$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = a + b\n",
    "d = b + 1\n",
    "print('c=%s, d=%s' % (c.data[0], d.data[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we calculate the output $e = cd$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = c * d\n",
    "print(e.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, if we want to calculate the derivatives, $\\frac{\\partial e}{\\partial a}$ and $\\frac{\\partial e}{\\partial b}$, we just call $\\texttt{e.backward()}$. Are the values outputted below what you would expect?\n",
    "\n",
    "<img src=\"tutorial_img/comp_graph_d.png\" width=\"50%\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we had $c = a+b$, $d = b+1$, and $e = c*d$. In order the calculate $\\frac{\\partial e}{\\partial a}$ and $\\frac{\\partial e}{\\partial b}$, we need to use the chain rule in calculus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write your answers for problems 1 to 3 in terms of $a, b, c, d, e$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1\n",
    "What are the partial derivatives $\\Large\\frac{\\partial e}{\\partial c}$, $\\Large\\frac{\\partial e}{\\partial d}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large\\frac{\\partial e}{\\partial c} = $\n",
    "\n",
    "$\\Large\\frac{\\partial e}{\\partial d} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2\n",
    "What are the partial derivatives $\\Large\\frac{\\partial c}{\\partial a}$, $\\Large\\frac{\\partial c}{\\partial b}$, $\\Large\\frac{\\partial d}{\\partial b}$? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large\\frac{\\partial c}{\\partial a} = $\n",
    "\n",
    "$\\Large\\frac{\\partial c}{\\partial b} = $\n",
    "\n",
    "$\\Large\\frac{\\partial d}{\\partial b} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "Now use the chain rule and your answers from previous parts to find $\\Large\\frac{\\partial e}{\\partial a}$ and $\\Large\\frac{\\partial e}{\\partial b}$. Note that the gradient for $b$ is coming in from two different signals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large\\frac{\\partial e}{\\partial a} = $\n",
    "\n",
    "$\\Large\\frac{\\partial e}{\\partial b} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4\n",
    "Now, what are $\\Large\\frac{\\partial e}{\\partial a}$ and $\\Large\\frac{\\partial e}{\\partial b}$ given our previous inptus $a=2$, $b=1$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\Large\\frac{\\partial e}{\\partial a} = $\n",
    "\n",
    "$\\Large\\frac{\\partial e}{\\partial b} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See if your calculations match the output below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('de/da', a.grad.data[0])\n",
    "print('de/db', b.grad.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what autograd is doing! It is constructing a computational graph that uses chain rule to calculate the derivates of the output with respect to all of its inputs!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example\n",
    "Here is another example with the input as a $2\\times 2$ matrix, where $x = \\begin{bmatrix}x_1&x_2\\\\x_3&x_4\\end{bmatrix} = \\begin{bmatrix}1&1\\\\1&1\\end{bmatrix}$. And our computational graph is modelling the function $3(x+2)^2$, where the square is a elementwise square of the matrix. More specifically, we have\n",
    "$$\\begin{bmatrix}3(x_1+2)^2&3(x_2+2)^2\\\\3(x_3+2)^2&3(x_4+2)^2\\end{bmatrix}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = Variable(torch.ones(2, 2), requires_grad=True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do an operation of variable:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``y`` was created as a result of an operation, so it has a ``grad_fn``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do more operations on y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = y * y * 3\n",
    "out = z.mean()\n",
    "\n",
    "print(z, out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradients\n",
    "---------\n",
    "let's backprop now\n",
    "``out.backward()`` is equivalent to doing ``out.backward(torch.Tensor([1.0]))``\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print gradients d(out)/dx\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should have got a matrix of ``4.5``. Let’s call the ``out``\n",
    "*Variable* “$o$”.\n",
    "We have that $o = \\frac{1}{4}\\sum_i z_i$,\n",
    "$z_i = 3(x_i+2)^2$ and $z_i\\bigr\\rvert_{x_i=1} = 27$.\n",
    "Therefore,\n",
    "$\\frac{\\partial o}{\\partial x_i} = \\frac{3}{2}(x_i+2)$, hence\n",
    "$\\frac{\\partial o}{\\partial x_i}\\bigr\\rvert_{x_i=1} = \\frac{9}{2} = 4.5$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can do many crazy things with autograd!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(3)\n",
    "x = Variable(x, requires_grad=True)\n",
    "\n",
    "y = x * 2\n",
    "while y.data.norm() < 1000:\n",
    "    y = y * 2\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradients = torch.FloatTensor([0.1, 1.0, 0.0001])\n",
    "y.backward(gradients)\n",
    "\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{Instructions}$: Complete the exercises below. Make sure to only use basic operations (+, -, *, etc) or torch functions when doing tensor calculations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1\n",
    "\n",
    "Create computational graph with $x$ as input, and output as $max(x, 0)$. This is the ReLU activation which is commonly used instead of the sigmoid in neural nets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1(x):\n",
    "    zero = Variable(torch.FloatTensor([0])) # use this as the zero in max(x, 0)\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "    return None\n",
    "    \n",
    "def check_q1():\n",
    "    x1 = Variable(torch.FloatTensor([-1]), requires_grad=True)\n",
    "    x2 = Variable(torch.FloatTensor([1]), requires_grad=True)\n",
    "    assert q1(x1).data[0] == 0\n",
    "    assert q1(x2).data[0] == 1\n",
    "    print('Passed')\n",
    "check_q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write a function that prints out the gradient of $max(x,0)$ at the given $x$. You may need to call your function $\\texttt{q1(x)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_q1(x):\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "\n",
    "def check_grad_q1():\n",
    "    x1 = Variable(torch.FloatTensor([-10]), requires_grad=True)\n",
    "    x2 = Variable(torch.FloatTensor([100]), requires_grad=True)\n",
    "    assert grad_q1(x1).data[0] == 0\n",
    "    assert grad_q1(x2).data[0] == 1\n",
    "    print('Passed')\n",
    "check_grad_q1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Find the derivative of the function $f(x, y, z) = \\frac{(x-1)(y-2)^2(y-3)^3}{\\sqrt{x+y+z}}$ with respect to $x, y$, and $z$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q2(x, y, z): # The inputs are all torch Variable objects\n",
    "    # 1. Calculate f(x,y,z)\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "    # 2. Find and return df/dx, df/dy, df/dz as x, y, and z (in that order)\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "    return None\n",
    "\n",
    "def check_q2():\n",
    "    import numpy as np\n",
    "    x = Variable(torch.FloatTensor([4]), requires_grad=True)\n",
    "    y = Variable(torch.FloatTensor([5]), requires_grad=True)\n",
    "    z = Variable(torch.FloatTensor([6]), requires_grad=True)\n",
    "    \n",
    "    x_grad, y_grad, z_grad = q2(x, y, z)\n",
    "    x_grad, y_grad, z_grad = x_grad.data[0], y_grad.data[0], z_grad.data[0]\n",
    "    \n",
    "    assert np.allclose(x_grad, 56.46809387207031)\n",
    "    assert np.allclose(y_grad, 119.21041870117188)\n",
    "    assert np.allclose(z_grad, 181.95274353027344)\n",
    "    \n",
    "    print('Passed')\n",
    "    \n",
    "check_q2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3\n",
    "Find the derivative of the perceptron with respect to each of its inputs in $x$ (The perceptron is the same thing as the neuron you created in the NeuralNetwork notebook, just this time with torch objects and gradients). There is no activation function. $\\textbf{Important: Use torch.randn to generate your weights and bias.}$\n",
    "\n",
    "Make sure you first generate your weights (vector) $\\textit{and then}$ bias (scalar), or else you might fail to test even though you have a correct implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x is a 5 dimensional vector Variable object (no need to create a Variable, it will be already created)\n",
    "def q3(x):\n",
    "    # 1. Calculate f(x)\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "    # 2. Find and return df/dx, df/dw, df/b - Which ones of these are vectors? (Make sure you know/understand this)\n",
    "    # Remember that if you want to derivative wrt w, b, they need to be Variable objects\n",
    "    \"\"\" Your Code Here \"\"\"\n",
    "    return None\n",
    "\n",
    "def check_q3():\n",
    "    import numpy as np\n",
    "    torch.manual_seed(0)\n",
    "    x = Variable(torch.randn(5), requires_grad=True)\n",
    "    x_grad, w_grad, b_grad = q3(x)\n",
    "    x_grad, w_grad, b_grad = x_grad.data, w_grad.data, b_grad.data\n",
    "    x_grad, w_grad, b_grad = x_grad.numpy(), w_grad.numpy(), b_grad.numpy()\n",
    "    \n",
    "    xg_correct = [-1.3985955, 0.40334684, 0.83802634, -0.7192576, -0.40334353]\n",
    "    wg_correct = [1.5409961, -0.2934289, -2.1787894, 0.56843126, -1.0845224]\n",
    "    bg_correct = 1\n",
    "    assert np.allclose(x_grad, xg_correct)\n",
    "    assert np.allclose(w_grad, wg_correct)\n",
    "    assert np.allclose(b_grad, bg_correct)\n",
    "    print('Passed')\n",
    "    \n",
    "    \n",
    "check_q3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
