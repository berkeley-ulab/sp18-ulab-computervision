{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gram Matrix\n",
    "\n",
    "Consider a matrix $X$ that is $n\\times d$. Then the Gram Matrix, $G$, is defined as $XX^T$. In order to get some motivation behind this matrix, let's look at $X$ in terms of its row vectors.\n",
    "$$X = \\begin{bmatrix}\\vec{x}^T_1 \\\\  \\vec{x}^T_2 \\\\ \\vdots \\\\ \\vec{x}^T_n\\end{bmatrix}$$\n",
    "where each $\\vec{x}_i$ is a $d$-dimensional vector. Then, we have\n",
    "\\begin{align*}G = XX^T &= \\begin{bmatrix}\\vec{x}^T_1 \\\\  \\vec{x}^T_2 \\\\ \\vdots \\\\ \\vec{x}_n\\end{bmatrix}\\begin{bmatrix}\\vec{x}_1 &  \\vec{x}_2 & \\dots & \\vec{x}_n\\end{bmatrix} \\\\\n",
    "&= \\begin{bmatrix}x_1^Tx_1 & \\dots & x_1^Tx_n \\\\ \\vdots & \\ddots & \\vdots \\\\ x_n^Tx_1 & \\dots & x_n^Tx_n\\end{bmatrix}\\end{align*}\n",
    "So the Gram Matrix is just a matrix of all combinations of dot products for the rows of $X$. The dot product can be seen as a measure of similarity (projections) between row vectors. In addition, because all rows are multiplied with each other, we can see this as a distribution of the spatial information, so the Gram Matrix contains information on the style and texture of the image!\n",
    "\n",
    "Let us know if you have any questions about this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise \n",
    "\n",
    "Calculate the Gram Matrix for $X$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gram_matrix(X):\n",
    "    return torch.mm(X, X.t())\n",
    "\n",
    "def test_calculate_gram_matrix():\n",
    "    torch.manual_seed(0)\n",
    "    X = torch.randn(100, 50)\n",
    "    G = calculate_gram_matrix(X)\n",
    "    G_test = np.load('test_data/calculate_gram_matrix_test.npy')\n",
    "    assert np.allclose(G.numpy(), G_test)\n",
    "    \n",
    "test_calculate_gram_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "In machine learning, we usually run batches of data through our model instead of running each training example one at a time. If a CNN was designed to receive $C\\times W\\times H$ images, then we input multiple images at a time but giving the model $B \\times C \\times W \\times H$, where $B$ is our batch size. Suppose we had a tensor that was $B\\times M\\times N$. Calculate the Gram matrix each example in the given batch. Your output should be of size $B\\times M \\times M$ - $B$ separate Gram matrices. Hint: see torch.bmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_gram_matrix(X_batch):\n",
    "    \"\"\" You Code Here\"\"\"\n",
    "\n",
    "def test_batch_gram_matrix():\n",
    "    torch.manual_seed(0)\n",
    "    X_batch = torch.randn(10, 100, 50)\n",
    "    G_batch = batch_gram_matrix(X_batch)\n",
    "    G_batch_test = np.load('test_data/batch_gram_matrix_test.npy')\n",
    "    assert np.allclose(G_batch.numpy(), G_batch_test)\n",
    "\n",
    "test_batch_gram_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you should have all the tools you need to implement style transfer! Go to the `style-transfer-original` folder, which will have everything you need. Open up `StyleTransfer.ipynb` in `style-transfer-original`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ulab]",
   "language": "python",
   "name": "conda-env-ulab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
