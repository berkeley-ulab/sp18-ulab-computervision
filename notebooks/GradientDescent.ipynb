{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
<<<<<<< HEAD
=======
    "import matplotlib.pyplot as plt\n",
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
    "import style_transfer.vgg as vgg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "We learned in the previous notebook how you can use the Pytorch autograd package to calculate the gradients of variables. In this notebook we will go over key concepts in how gradients are used in style transfer. This will include:\n",
    "\n",
    "* Models\n",
    "* Loss functions\n",
    "* Gradient descent\n",
    "\n",
<<<<<<< HEAD
    "We will focus on loss functions and gradient descent. For the purpose of this notebook, we will treat the model as a black box with only a brief overview of what they are."
=======
    "We will focus on loss functions and gradient descent. For the purpose of this notebook, we will treat the VGG neural network as a black box with only a brief overview of what they are. \n",
    "\n",
    "We'll call it a \"model\".\n",
    "\n",
    "In deep learning, a computer model learns information* directly from images, text, or sound. Deep learning models can achieve state-of-the-art accuracy, sometimes exceeding human-level performance. Models are trained by using a large set of labeled data and neural network architectures that contain many layers.\n",
    "\n"
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vgg_model, style_image, content_image, opt_img = vgg.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss Functions\n",
    "\n",
    "We have:\n",
    "- $\\texttt{vgg}$: We will not be focusing on what $\\texttt{vgg}$ is in this notebook, but we will provide a general overview. $\\texttt{vgg}$ takes in an input image ($\\texttt{opt_img}$) and feeds it through sequential layers of the model. The output of the previous layer is the input of the next layer, and so on. The main purpose of $\\texttt{vgg}$ is to help quantify the style of $\\texttt{opt_img}$.\n",
<<<<<<< HEAD
    "- $\\texttt{content_img}$: We want the end result to be similar in $\\textbf{content}$ with this image\n",
    "- $\\texttt{style_img}$: We want the end result to be similar in $\\textbf{style}$ with this image.\n",
=======
    "- $\\texttt{content_img}$: We want the end result to be similar in $\\textbf{content}$ with this image (i.e. houses, river)\n",
    "\n",
    "<img src=\"Images/Tuebingen_Neckarfront.jpg\" width=\"423\">\n",
    "<img src=\"Images/vangogh_starry_night.jpg\" width=\"400\">\n",
    "\n",
    "\n",
    "- $\\texttt{style_img}$: We want the end result to be similar in $\\textbf{style}$ with this image. (i.e. impressionist  style, curly sky)\n",
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
    "- $\\texttt{opt_img}$: The image we are trying to change so that it fits both the content of $\\texttt{content_img}$ and the style of $\\texttt{style_img}$\n",
    "\n",
    "Style transfer is an iterative process. Every iteration, we adjust the image slightly to be closer to the style of the style image, while retaining most of the content. Now, how do we quantify how close $\\texttt{opt_img}$ is compared to the style and content images?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1\n",
    "\n",
<<<<<<< HEAD
    "Given two images, A and B of the same size (i.e. both 28 x 28). Can you think of a method to calculate the distance between both images? Your solution doesn't have to be exact, it just needs to give a rough idea of the difference between two images. $\\textit{Hint: What are different ways to measure the distance between two vectors?}$"
=======
    "Given two images, A and B of the same size (i.e. both 28 x 28 pixels). Can you think of a method to calculate the distance between both images? Your solution doesn't have to be exact. The distance should be able to quantify some kind of difference between two images. $\\textit{Hint: What are different ways to measure the distance between two vectors?}$"
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Describe your method here: ANSWER HERE\n",
    "\"\"\"\n",
    "\n",
    "def dist(A, B):\n",
    "    \"\"\" Your Code Here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.zeros(28, 28)\n",
    "B = torch.zeros(28, 28)\n",
    "print(dist(A, B))\n",
    "\n",
    "C = torch.ones(28, 28)\n",
    "print(dist(B, C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "A common way to measure the difference between any two matrices is to use Mean-Squared Error (MSE). In our case, we want to minimize the distance between $A$ and $B$, so distance is the same as loss (or error). Let $A$, $B$ be two $m\\times n$, then\n",
    "$$\\text{MSE} = \\frac{1}{mn}\\sum_{i=1}^m \\sum_{j=1}^n (A_{ij}-B_{ij})^2$$\n",
    "It gives a general sense of distance in that it measure the average distance between each corresponding value of $A$ and $B$"
=======
    "A common way to measure the difference between any two matrices of the same size is to use Mean-Squared Error (MSE). In our case, we want to minimize the distance between our starting image $\\texttt{opt_img}$ and the \"true\" stylized image, so distance between these two images is the same as loss (or error). Let $A$, $B$ be two $m\\times n$ matrices, then\n",
    "$$\\text{MSE} = \\frac{1}{mn}\\sum_{i=1}^m \\sum_{j=1}^n (A_{ij}-B_{ij})^2$$\n",
    "MSE gives a general sense of distance in that it measure the average distance between each corresponding entry of $A$ and $B$."
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2\n",
    "Given two matrices $A$, $B$, calculate the MSE (as defined above) without using any for loops. Note that the input is two torch tensors, so you should be using torch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mse(A, B):\n",
    "    \"\"\" Your Code Here \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = 10, 20\n",
    "A, B = torch.rand(m, n), torch.rand(m, n)\n",
    "actual = calculate_mse(A, B)\n",
    "\n",
    "expected = sum([sum([(A[i, j] - B[i, j]) ** 2 for j in range(n)]) for i in range(m)])\n",
    "\n",
    "if np.allclose(actual, expected):\n",
    "    print('Passed')\n",
    "else:\n",
    "    print('Failed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
<<<<<<< HEAD
    "Now, let's take a look at a part of the style transfer code. $\\texttt{nn.MSELoss}$ does exactly what we calculated above. It takes as input 2 matrices, and calculates the MSE between them. In this case, $\\texttt{GramMSELoss}$ calculates the MSE between a Gram matrix and a target Gram matrix. The Gram Matrix is a way to measure the style of an image. (We will get more in-depth later)"
=======
    "Now, let's take a look at a part of the style transfer code. $\\texttt{nn.MSELoss}$ does exactly what we calculated above. It takes as input 2 matrices, and calculates the MSE between them. \n",
    "\n",
    "\n",
    "#### The Gram Matrix is a way to measure the style of an image. (We will get more in-depth later)\n",
    "In this case, $\\texttt{GramMSELoss}$ calculates the MSE between a Gram matrix and a target Gram matrix. "
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(vgg.GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Loss Function for Style Transfer\n",
    "\n",
<<<<<<< HEAD
    "Remember that in style transfer, we want $\\texttt{img_opt}$ to be similar in style to the style image, but also retain the original content. So, our loss will consist of a combination of both.\n",
=======
    "Remember that in style transfer, we want $\\texttt{opt_img}$ to be similar in style to the style image, but also retain the original content. So, our loss will consist of a combination of both.\n",
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
    "$$\\text{Loss}_{total} = \\text{Loss}_{content} + \\text{Loss}_{style}$$\n",
    "\n",
    "The loss functions for style transfer are created in the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "content_layers = ['r42']\n",
    "loss_layers = style_layers + content_layers\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "if torch.cuda.is_available():\n",
    "    loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
    "    \n",
    "#these are good weights settings:\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "#compute optimization targets\n",
    "style_targets = [vgg.GramMatrix()(A).detach() for A in vgg_model(style_image, style_layers)]\n",
    "content_targets = [A.detach() for A in vgg_model(content_image, content_layers)]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent\n",
    "\n",
<<<<<<< HEAD
=======
    "To explain Gradient Descent we’ll use the classic mountaineering example.\n",
    "\n",
    "Suppose you are at the top of a mountain, and you have to reach a lake which is at the lowest point of the mountain (a.k.a valley). A twist is that you are blindfolded and you have zero visibility to see where you are headed. So, what approach will you take to reach the lake?\n",
    "\n",
    "<img src=\"tutorial_img/grad_desc1.png\">\n",
    "\n",
    "The best way is to check the ground near you and observe where the land tends to descend. This will give an idea in what direction you should take your first step. If you follow the descending path, it is very likely you would reach the lake. Now, to be more efficient, it would be nice to follow the steepest descent direction! From math, we know that the gradient (or slope) represents the steepness of the graph. Then, if we follow the opposite of direction of the gradient, then we can follow the steepest descent direction!\n",
    "\n",
    "<img src=\"tutorial_img/vector_calculus.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
    "Now that we have a way to measure the loss, the next step is the find some new $\\texttt{opt_img}$ that decreases the loss. We can do this by adjusting $\\texttt{opt_img}$ in the direction opposite of the gradient. In order to give some intuition into how gradient descent works, we will provide a short demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.linspace(-50, 50, 20)\n",
    "ys = xs * 5 + np.random.randn(len(xs)) * 50\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.scatter(xs, ys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to estimate a function $f(x) = wx$ that best fits the data. We define best fit by the model that minimizes the MSE, where\n",
    "$$MSE = \\frac{1}{20}\\sum_{i=1}^{20} (y_i - f(x_i))^2$$\n",
<<<<<<< HEAD
    "where $x_i = xs[i]$ and $y_i = ys[i]$. We want to find the optimal $w$ (slope) that minimizes the MSE. Note that we assume that the y-intercept is 0. Gradient descent is the process of iteratively modifying your parameters ($w$) to minimize some function - in this case MSE. "
=======
    "where $x_i = xs[i]$ and $y_i = ys[i]$. We want to find the optimal $w$ (slope) that minimizes the MSE. Note that we assume that the y-intercept is 0. Gradient descent is the process of iteratively modifying your parameters ($w$) to minimize some function - in this case MSE. The line we want to fit iteratively will progress like this (except in our case, we will fix the y-intercept to be 0).\n",
    "\n",
    "<img src=\"tutorial_img/gradient_descent_example.gif\">"
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(xs, w):\n",
    "    return xs * w\n",
    "\n",
    "def loss(xs, ys, w):\n",
    "    return 1/20 * np.sum((ys - f(xs, w)) ** 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3\n",
    "The derivative $\\frac{\\partial MSE}{\\partial w}$ can help tell us how to adjust $w$ in order to minimize loss. Calculate and implement the derivative of MSE. $\\textit{Hint: Use the above formula, substitute in $f(x_i) = wx_i$ and take the derivative with respect to $w$}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad(xs, ys, w):\n",
    "    \"\"\" Your Code Here - Try to vectorize your solution! (but it's alright if you dont)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_check():\n",
    "    delta = 1e-8\n",
    "    w = 2.5\n",
    "    actual_grad = grad(xs, ys, w)\n",
    "    num_grad = (loss(xs, ys, w+delta) - loss(xs, ys, w)) / delta\n",
    "    if np.allclose(num_grad, grad(xs, ys, w)):\n",
    "        print('Passed - Error: %s' % abs(actual_grad - num_grad))\n",
    "    else:\n",
    "        print('Failed - Error: %s' % abs(actual_grad - num_grad))\n",
    "grad_check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Execute the code below to show a plot of $w$ vs $MSE$. See how we want to change $w$ in the direction $\\textbf{opposite}$ to the derivative. We want to increase $w$ if the derivative is negative and decrease $w$ if it is positive. The classic analogy is rolling a ball down a valley - the ball will follow the steepest descent and eventually come to a stop at the base of the valley. This motivates the idea that our update is $w_{new} = w_{old} - \\alpha\\frac{\\partial MSE}{\\partial w}$, where $\\alpha$ is some small number to limit the amount that we step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = np.linspace(0,10,11)\n",
    "losses = [loss(xs, ys, w) for w in ws]\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('mse')\n",
    "plt.plot(ws, losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a demo to show you gradient descent at work. The figure on the left is the data + the current model you have. The figure on the right shows $w$ on the x-axis and loss on the y-axis. We want $w$ that minimizes loss, which is at the true $w = 5$. The points represent the current $w$ values of each iteration - see how it is approaching the minimum. Make sure you understand how the gradient descent code below works. You can ignore the parts about plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just some code to help with plotting - you can ignore this\n",
    "def update(xs, ys, w):\n",
    "    preds = xs * w\n",
    "    f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    plt.gcf()\n",
    "    ax1.set_xlabel('x')\n",
    "    ax1.set_ylabel('y')\n",
    "    ax1.scatter(xs, ys)\n",
    "    ax1.plot(xs, preds)\n",
    "    ax2.plot(ws, losses)\n",
    "    ax2.scatter(w_prev, loss_prev)\n",
    "    ax2.set_xlabel('w')\n",
    "    ax2.set_ylabel('loss (mse)')\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = 10\n",
    "w = 0\n",
    "\n",
    "w_prev = [w]\n",
    "loss_prev = [loss(xs, ys, w)]\n",
    "update(xs, ys, w)\n",
    "\n",
    "step_size = 0.0001 # Try changing this to 0.001, 0.01, 0.1, 1 and see what happens!\n",
    "\n",
    "for i in range(iters):\n",
    "    g = grad(xs, ys, w)\n",
    "    w = w - step_size * g \n",
    "    \n",
    "    w_prev.append(w)\n",
    "    loss_prev.append(loss(xs, ys, w))\n",
    "    update(xs, ys, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make sure you understand this next part! It is very important in Style Transfer\n",
    "\n",
    "In the previous example, all we did was iteratively adjust $w$ in the direction that minimized loss. However, what would would happen if we kept $w$ fixed and adjusted the $x$ values (our input) to minimize loss? Try running the demo below and see what happens!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_x(xs, ys, w):\n",
    "    # Calculates the derivaative of MSE with respect to each x-value, returns a vector of length 20 (one for each data point)\n",
    "    return -1/10 * (ys - w * xs) * w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "\n",
    "Can you explain what is happening in the demo below? How can you relate this to MSE and $f(x)$? What line is the data being fit to? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = 2.5\n",
    "xs = np.linspace(-50, 50, 20)\n",
    "ys = xs * 5 + np.random.randn(len(xs)) * 100\n",
    "step_size = 0.25\n",
    "\n",
    "for i in range(iters):\n",
    "    g = grad_x(xs, ys, w)\n",
    "    xs = xs - step_size * g \n",
    "    \n",
    "    preds = xs * w\n",
    "    fig = plt.figure()\n",
    "    plt.gcf()\n",
    "    plt.scatter(xs, ys)\n",
    "    display.clear_output(wait=True)\n",
    "    display.display(plt.gcf())\n",
    "    time.sleep(1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\textbf{This is exactly what style transfer is doing! We are trying to optimize our input image to decrease the loss}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent in Style Transfer\n",
    "\n",
    "In style transfer, we will be performing gradient descent on $\\texttt{opt_img}$ by treating the image as a set of parameters. One iteration of the gradient descent code is below. Can you identifty the different parts of gradient descent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd():\n",
    "    def closure():\n",
    "        optimizer.zero_grad() # Zeros out all the gradients - this is necessary because gradients from previous iterations will accumulate\n",
    "        out = vgg(opt_img, loss_layers) # Run \n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)] # Calculate the loss\n",
    "        loss = sum(layer_losses) # Calculate the loss\n",
    "        loss.backward() # Calculate the gradients\n",
    "        n_iter[0]+=1\n",
    "        if n_iter[0]%show_iter == (show_iter-1):\n",
    "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data[0]))\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure) # Adjust the parameters in the direction of decreasing loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "show_iter = 2\n",
    "optimizer = optim.LBFGS([opt_img]); # The parameters are the values of the input image\n",
    "n_iter=[0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "    gd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please let us know if you have any questions about the gradient descent code above!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
<<<<<<< HEAD
  "kernelspec": {
   "display_name": "Python [conda env:ulab]",
   "language": "python",
   "name": "conda-env-ulab-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
=======
  "language_info": {
   "name": "python",
   "pygments_lexer": "ipython3"
>>>>>>> f94455d651f37d220d4759040e8381d7b79df7ed
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
