{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer Demo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this semester is to:\n",
    "\n",
    "* Understand the state of the art in Style Transfer\n",
    "* Develop skills and tools necessary to do so\n",
    "\n",
    "This notebook, [inspired by this repo](https://github.com/leongatys/PytorchNeuralStyleTransfer), will be a quick glance into the internals of the original Style Transfer implementation. \n",
    "\n",
    "Don't worry if you don't know what's going on!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table>\n",
    "    <tr>\n",
    "    <td> <img src=\"./Images/vangogh_starry_night.jpg\" alt=\"Drawing\" style=\"width: 50px;\"/> </td>\n",
    "    <td> <img src=\"./Images/Tuebingen_Neckarfront.jpg\" alt=\"Drawing\" style=\"width: 50px;\"/> </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import time\n",
    "import os \n",
    "image_dir = os.getcwd() + '/Images/'\n",
    "model_dir = os.getcwd() + '/Models/'\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "\n",
    "from PIL import Image\n",
    "from collections import OrderedDict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download a pre-trained image classification weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert not os.path.exists(\"./Models/vgg_conv.pth\"), \"Not redownloading vgg...\"\n",
    "!mkdir Models && cd Models && wget -c --no-check-certificate https://bethgelab.org/media/uploads/pytorch_models/vgg_conv.pth && cd .."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define placeholders to put weights in a way that conveniently lets you grab the outputs from any layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class VGG(nn.Module):\n",
    "    def __init__(self, pool='max'):\n",
    "        super(VGG, self).__init__()\n",
    "        #vgg modules\n",
    "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=3, padding=1)\n",
    "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=3, padding=1)\n",
    "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=3, padding=1)\n",
    "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv3_4 = nn.Conv2d(256, 256, kernel_size=3, padding=1)\n",
    "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv4_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        self.conv5_4 = nn.Conv2d(512, 512, kernel_size=3, padding=1)\n",
    "        if pool == 'max':\n",
    "            self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        elif pool == 'avg':\n",
    "            self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool3 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool4 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            self.pool5 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "            \n",
    "    def forward(self, x, out_keys):\n",
    "        out = {}\n",
    "        out['r11'] = F.relu(self.conv1_1(x))\n",
    "        out['r12'] = F.relu(self.conv1_2(out['r11']))\n",
    "        out['p1'] = self.pool1(out['r12'])\n",
    "        out['r21'] = F.relu(self.conv2_1(out['p1']))\n",
    "        out['r22'] = F.relu(self.conv2_2(out['r21']))\n",
    "        out['p2'] = self.pool2(out['r22'])\n",
    "        out['r31'] = F.relu(self.conv3_1(out['p2']))\n",
    "        out['r32'] = F.relu(self.conv3_2(out['r31']))\n",
    "        out['r33'] = F.relu(self.conv3_3(out['r32']))\n",
    "        out['r34'] = F.relu(self.conv3_4(out['r33']))\n",
    "        out['p3'] = self.pool3(out['r34'])\n",
    "        out['r41'] = F.relu(self.conv4_1(out['p3']))\n",
    "        out['r42'] = F.relu(self.conv4_2(out['r41']))\n",
    "        out['r43'] = F.relu(self.conv4_3(out['r42']))\n",
    "        out['r44'] = F.relu(self.conv4_4(out['r43']))\n",
    "        out['p4'] = self.pool4(out['r44'])\n",
    "        out['r51'] = F.relu(self.conv5_1(out['p4']))\n",
    "        out['r52'] = F.relu(self.conv5_2(out['r51']))\n",
    "        out['r53'] = F.relu(self.conv5_3(out['r52']))\n",
    "        out['r54'] = F.relu(self.conv5_4(out['r53']))\n",
    "        out['p5'] = self.pool5(out['r54'])\n",
    "        return [out[key] for key in out_keys]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the weights into the placeholders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vgg = VGG()\n",
    "vgg.load_state_dict(torch.load(model_dir + 'vgg_conv.pth'))\n",
    "for param in vgg.parameters():\n",
    "    param.requires_grad = False\n",
    "if torch.cuda.is_available():\n",
    "    vgg.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# gram matrix and loss\n",
    "class GramMatrix(nn.Module):\n",
    "    def forward(self, input):\n",
    "        b,c,h,w = input.size()\n",
    "        F = input.view(b, c, h*w)\n",
    "        G = torch.bmm(F, F.transpose(1,2)) \n",
    "        G.div_(h*w)\n",
    "        return G\n",
    "\n",
    "class GramMSELoss(nn.Module):\n",
    "    def forward(self, input, target):\n",
    "        out = nn.MSELoss()(GramMatrix()(input), target)\n",
    "        return(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup pre and post processing for images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_size = 512 \n",
    "prep = transforms.Compose([transforms.Scale(img_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to BGR\n",
    "                           transforms.Normalize(mean=[0.40760392, 0.45795686, 0.48501961], #subtract imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x.mul_(255)),\n",
    "                          ])\n",
    "postpa = transforms.Compose([transforms.Lambda(lambda x: x.mul_(1./255)),\n",
    "                           transforms.Normalize(mean=[-0.40760392, -0.45795686, -0.48501961], #add imagenet mean\n",
    "                                                std=[1,1,1]),\n",
    "                           transforms.Lambda(lambda x: x[torch.LongTensor([2,1,0])]), #turn to RGB\n",
    "                           ])\n",
    "postpb = transforms.Compose([transforms.ToPILImage()])\n",
    "def postp(tensor): # to clip results in the range [0,1]\n",
    "    t = postpa(tensor)\n",
    "    t[t>1] = 1    \n",
    "    t[t<0] = 0\n",
    "    img = postpb(t)\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#load images, ordered as [style_image, content_image]\n",
    "img_dirs = [image_dir, image_dir]\n",
    "img_names = ['vangogh_starry_night.jpg', 'Tuebingen_Neckarfront.jpg']\n",
    "imgs = [Image.open(img_dirs[i] + name) for i,name in enumerate(img_names)]\n",
    "imgs_torch = [prep(img) for img in imgs]\n",
    "if torch.cuda.is_available():\n",
    "    imgs_torch = [Variable(img.unsqueeze(0).cuda()) for img in imgs_torch]\n",
    "else:\n",
    "    imgs_torch = [Variable(img.unsqueeze(0)) for img in imgs_torch]\n",
    "style_image, content_image = imgs_torch\n",
    "\n",
    "# opt_img = Variable(torch.randn(content_image.size()).type_as(content_image.data), requires_grad=True) #random init\n",
    "opt_img = Variable(content_image.data.clone(), requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define layers, loss functions, weights and compute optimization targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "style_layers = ['r11','r21','r31','r41', 'r51'] \n",
    "content_layers = ['r42']\n",
    "loss_layers = style_layers + content_layers\n",
    "loss_fns = [GramMSELoss()] * len(style_layers) + [nn.MSELoss()] * len(content_layers)\n",
    "if torch.cuda.is_available():\n",
    "    loss_fns = [loss_fn.cuda() for loss_fn in loss_fns]\n",
    "    \n",
    "#these are good weights settings:\n",
    "style_weights = [1e3/n**2 for n in [64,128,256,512,512]]\n",
    "content_weights = [1e0]\n",
    "weights = style_weights + content_weights\n",
    "\n",
    "#compute optimization targets\n",
    "style_targets = [GramMatrix()(A).detach() for A in vgg(style_image, style_layers)]\n",
    "content_targets = [A.detach() for A in vgg(content_image, content_layers)]\n",
    "targets = style_targets + content_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "max_iter = 50\n",
    "show_iter = 2\n",
    "optimizer = optim.LBFGS([opt_img]);\n",
    "n_iter=[0]\n",
    "\n",
    "while n_iter[0] <= max_iter:\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        out = vgg(opt_img, loss_layers)\n",
    "        layer_losses = [weights[a] * loss_fns[a](A, targets[a]) for a,A in enumerate(out)]\n",
    "        loss = sum(layer_losses)\n",
    "        loss.backward()\n",
    "        n_iter[0]+=1\n",
    "        #print loss\n",
    "        if n_iter[0]%show_iter == (show_iter-1):\n",
    "            print('Iteration: %d, loss: %f'%(n_iter[0]+1, loss.data[0]))\n",
    "#             print([loss_layers[li] + ': ' +  str(l.data[0]) for li,l in enumerate(layer_losses)]) #loss of each layer\n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "#display result\n",
    "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_img = postp(opt_img.data[0].cpu().squeeze())\n",
    "imshow(out_img)\n",
    "gcf().set_size_inches(10,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
